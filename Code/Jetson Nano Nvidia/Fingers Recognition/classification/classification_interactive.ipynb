{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finger Counting Using Computer Vision\n",
    "\n",
    "## Introduction\n",
    "This project aims to develop an intelligent system capable of counting the number of fingers on a human hand in real-time. The goal is to display this count in an intuitive way using a binary display on LEDs, controlled via an ESP32 microcontroller. This task involves the use of advanced computer vision and artificial intelligence technologies, including leveraging the powerful Jetson Nano to run a deep learning model.\n",
    "\n",
    "## Context\n",
    "Microcontrollers such as the Jetson Nano and ESP32 are at the heart of many modern applications in robotics and the Internet of Things (IoT). They are capable of processing complex signals and controlling devices in real-time. The integration of artificial intelligence into these systems paves the way for more intuitive and interactive applications.\n",
    "\n",
    "## Problem Statement\n",
    "Recognizing human gestures is a complex issue due to the variability of hand shapes and lighting conditions. The challenge is to develop a system that can understand these visual signals and translate them into a simple digital format that is easy for users to interpret.\n",
    "\n",
    "## Project Development\n",
    "### Model Architecture\n",
    "For this project, we used a deep learning model based on ResNet-34, known for its performance in image classification tasks. This model was trained directly on the Jetson Nano, taking advantage of its GPU computing capabilities to speed up the process.\n",
    "\n",
    "### Training and Model Performance\n",
    "The model was trained on a dataset consisting of images of hands displaying a varying number of fingers. With the Jetson Nano's computational power, the model achieved satisfactory accuracy, demonstrating its ability to count fingers in real-time.\n",
    "\n",
    "### Integration with ESP32\n",
    "The trained model is used to perform inference on the Jetson Nano. The results are then transmitted to the ESP32 via an I2C connection, where they are converted into a binary display on LEDs.\n",
    "\n",
    "## Results and Discussion\n",
    "Tests performed show that the system can count fingers with high accuracy in real-time. The results are successfully displayed on the ESP32, validating the chosen approach.\n",
    "\n",
    "## Dataset and Model Training\n",
    "The initial dataset utilized in this project comprised exclusively of left-hand images, which provided a foundational understanding for the model. To enhance the model's robustness and increase its accuracy, the dataset was augmented with additional photos captured in a controlled environment. These supplementary images were crucial in training the model to recognize various finger counts and hand orientations. However, it is important to note that the current limitation of the model is its specificity to the left hand in certain orientations. Future iterations of the model could benefit from a more diverse dataset that includes multiple hand orientations and both left and right hands to improve generalizability.\n",
    "\n",
    "## Challenges and Limitations\n",
    "One of the key challenges faced during the project was the model's initial inability to generalize well to different hand orientations and lighting conditions. This was mitigated by augmenting the dataset with various images of the left hand. Despite improvements, the model still primarily recognizes the left hand within a certain orientation range. Further work is needed to extend the model's capabilities to include a wider variety of hand positions and to ensure robust recognition regardless of the hand used.\n",
    "\n",
    "Here are the gestures that the current model recognize :\n",
    "\n",
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>\n",
    "<center><img src=\"../images/rubblescout.png\" alt=\"Header\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "## Future Work\n",
    "To overcome the current limitations, it is proposed that future work should include collecting a balanced dataset that contains images of both left and right hands in multiple orientations and under diverse lighting conditions. Additionally, applying techniques such as data augmentation and transfer learning may further enhance the model's performance and its ability to operate in real-world scenarios.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "This project demonstrates the effectiveness of intelligent embedded systems in recognizing and interpreting human gestures. The successful integration of computer vision and electronics opens the way for innovative applications in robotics and IoT.\n",
    "\n",
    "## References\n",
    "- He K., Zhang X., Ren S., and Sun J., \"Deep Residual Learning for Image Recognition\", 2016.\n",
    "- Jetson Nano Developer Kit, NVIDIA.\n",
    "- ESP32, Espressif Systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with AI on Jetson Nano\n",
    "### Interactive Classification Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an interactive data collection, training, and testing tool, provided as part of the NVIDIA Deep Learning Institute (DLI) course, \"Getting Started with AI on Jetson Nano\". It is designed to be run on the Jetson Nano in conjunction with the detailed instructions provided in the online DLI course pages. \n",
    "\n",
    "To start the tool, set the **Camera** and **Task** code cell definitions, then execute all cells.  The interactive tool widgets at the bottom of the notebook will display.  The tool can then be used to gather data, add data, train data, and test data in an iterative and interactive fashion! \n",
    "\n",
    "The explanations in this notebook are intentionally minimal to provide a streamlined experience.  Please see the DLI course pages for detailed information on tool operation and project creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera\n",
    "First, create your camera and set it to `running`.  Uncomment the appropriate camera selection lines, depending on which type of camera you're using (USB or CSI). This cell may take several seconds to execute.\n",
    "\n",
    "<div style=\"border:2px solid black; background-color:#e3ffb3; font-size:12px; padding:8px; margin-top: auto;\">\n",
    "    <h4><i>Tip</i></h4>\n",
    "    <p>There can only be one instance of CSICamera or USBCamera at a time.  Before starting this notebook, make sure you have executed the final \"shutdown\" cell in any other notebooks you have run so that the camera is released. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crw-rw---- 1 root video 81, 0 Apr 16 14:38 /dev/video0\n"
     ]
    }
   ],
   "source": [
    "# Check device number\n",
    "!ls -ltrh /dev/video*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera created\n"
     ]
    }
   ],
   "source": [
    "from jetcam.usb_camera import USBCamera\n",
    "from jetcam.csi_camera import CSICamera\n",
    "\n",
    "# for USB Camera (Logitech C270 webcam), uncomment the following line\n",
    "camera = USBCamera(width=224, height=224, capture_device=0) # confirm the capture_device number\n",
    "\n",
    "# for CSI Camera (Raspberry Pi Camera Module V2), uncomment the following line\n",
    "# camera = CSICamera(width=224, height=224, capture_device=0) # confirm the capture_device number\n",
    "\n",
    "camera.running = True\n",
    "print(\"camera created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Next, define your project `TASK` and what `CATEGORIES` of data you will collect.  You may optionally define space for multiple `DATASETS` with names of your choosing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment/edit the associated lines for the classification task you're building and execute the cell.\n",
    "This cell should only take a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fingers task with ['0', '1', '2', '3', '4', '5'] categories defined\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from dataset import ImageClassificationDataset\n",
    "\n",
    "TASK = 'fingers'\n",
    "\n",
    "CATEGORIES = ['0','1', '2', '3', '4', '5']\n",
    "\n",
    "DATASETS = ['A']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = ImageClassificationDataset('../data/classification/' + TASK + '_' + name, CATEGORIES, TRANSFORMS)\n",
    "    \n",
    "print(\"{} task with {} categories defined\".format(TASK, CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data directory location if not there already\n",
    "DATA_DIR = '/nvdli-nano/data/classification/'\n",
    "!mkdir -p {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "Execute the cell below to create the data collection tool widget. This cell should only take a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_collection_widget created\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "# initialize active dataset\n",
    "dataset = datasets[DATASETS[0]]\n",
    "\n",
    "# unobserve all callbacks from camera in case we are running this cell for second time\n",
    "camera.unobserve_all()\n",
    "\n",
    "# create image preview\n",
    "camera_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# create widgets\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
    "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "save_widget = ipywidgets.Button(description='add')\n",
    "\n",
    "# manually update counts at initialization\n",
    "count_widget.value = dataset.get_count(category_widget.value)\n",
    "\n",
    "# sets the active dataset\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "# update counts when we select a new category\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "category_widget.observe(update_counts, names='value')\n",
    "\n",
    "# save image for category and update counts\n",
    "def save(c):\n",
    "    dataset.save_entry(camera.value, category_widget.value)\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "save_widget.on_click(save)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget]), dataset_widget, category_widget, count_widget, save_widget\n",
    "])\n",
    "\n",
    "# display(data_collection_widget)\n",
    "print(\"data_collection_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Execute the following cell to define the neural network and adjust the fully connected layer (`fc`) to match the outputs required for the project.  This cell may take several seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model configured and model_widget created\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# RESNET 18\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "#model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "\n",
    "# RESNET 34\n",
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='/nvdli-nano/data/classification/my_model.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "# display(model_widget)\n",
    "print(\"model configured and model_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live  Execution\n",
    "Execute the cell below to set up the live execution widget.  This cell should only take a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "live_execution_widget created\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from utils import preprocess\n",
    "import torch.nn.functional as F\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "prediction_widget = ipywidgets.Text(description='prediction')\n",
    "score_widgets = []\n",
    "for category in dataset.categories:\n",
    "    score_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, description=category, orientation='vertical')\n",
    "    score_widgets.append(score_widget)\n",
    "\n",
    "def live(state_widget, model, camera, prediction_widget, score_widget):\n",
    "    global dataset\n",
    "    while state_widget.value == 'live':\n",
    "        image = camera.value\n",
    "        preprocessed = preprocess(image)\n",
    "        output = model(preprocessed)\n",
    "        output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()\n",
    "        category_index = output.argmax()\n",
    "        prediction_widget.value = dataset.categories[category_index]\n",
    "        for i, score in enumerate(list(output)):\n",
    "            score_widgets[i].value = score\n",
    "            \n",
    "def start_live(change):\n",
    "    if change['new'] == 'live':\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget, score_widget))\n",
    "        execute_thread.start()\n",
    "\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox(score_widgets),\n",
    "    prediction_widget,\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "# display(live_execution_widget)\n",
    "print(\"live_execution_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Execute the following cell to define the trainer, and the widget to control it. This cell may take several seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer configured and train_eval_widget created\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "accuracy_widget = ipywidgets.FloatText(description='accuracy')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        state_widget.value = 'stop'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, labels in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                error_count += len(torch.nonzero(outputs.argmax(1) - labels).flatten())\n",
    "                count = len(labels.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                accuracy_widget.value = 1.0 - error_count / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        pass\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'live'\n",
    "    \n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "    \n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    accuracy_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "# display(train_eval_widget)\n",
    "print(\"trainer configured and train_eval_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Interactive Tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive tool includes widgets for data collection, training, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/classification_tool_key2.png\" alt=\"tool key\" width=500/></center>\n",
    "<br>\n",
    "<center><img src=\"../images/classification_tool_key1.png\" alt=\"tool key\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to create and display the full interactive widget.  Follow the instructions in the online DLI course pages to build your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c28a26e36e49d9b37eeecf29daf199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine all the widgets into one display\n",
    "all_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([data_collection_widget, live_execution_widget]), \n",
    "    train_eval_widget,\n",
    "    model_widget\n",
    "])\n",
    "\n",
    "display(all_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#76b900;\"></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you go...<br><br>Shut down the camera and/or notebook kernel to release the camera resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention!  Execute this cell before moving to another notebook\n",
    "# The USB camera application only requires that the notebook be reset\n",
    "# The CSI camera application requires that the 'camera' object be specifically released\n",
    "\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "if type(camera) is CSICamera:\n",
    "    print(\"Ignore 'Exception in thread' tracebacks\\n\")\n",
    "    camera.cap.release()\n",
    "\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to the DLI course pages for the next instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
